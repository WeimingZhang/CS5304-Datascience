{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext \n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pylab\n",
    "from sklearn.neighbors import KDTree\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data processing\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# English stop words list\n",
    "stop_words = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \\\n",
    "              \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\",\\\n",
    "              \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\\\n",
    "              \"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\\\n",
    "              \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\",\\\n",
    "              \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\",\\\n",
    "              \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\",\\\n",
    "              \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\",\\\n",
    "              \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\",\\\n",
    "              \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\",\\\n",
    "              \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\",\\\n",
    "              \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\",\\\n",
    "              \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\\\n",
    "              \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\",\\\n",
    "              \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\",\\\n",
    "              \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\",\\\n",
    "              \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\",\\\n",
    "              \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\\\n",
    "              \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\",\\\n",
    "              \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\",\\\n",
    "              \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\",\\\n",
    "              \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\",\\\n",
    "              \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\",\\\n",
    "              \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\",\\\n",
    "              \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\\\n",
    "              \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\",\\\n",
    "              \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\",\\\n",
    "              \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\",\\\n",
    "              \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\",\\\n",
    "              \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
    "\n",
    "# https://github.com/pararthshah/qa-memnn/blob/master/nltk_utils.py\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "'''\n",
    "takes in a list of tokens of length > 0\n",
    "returns a list of the most likely part of speech for the token\n",
    "\n",
    "'''\n",
    "def get_pos(tokens):\n",
    "    pos_list = nltk.pos_tag(tokens)\n",
    "    #print(pos_list)\n",
    "    pos_pegs = []\n",
    "    for i, (token, pos) in enumerate(pos_list):        \n",
    "        pos_peg = penn_to_wn(pos)\n",
    "        if pos_peg is not None:\n",
    "            pos_pegs.append(pos_peg)\n",
    "        else:\n",
    "            pos_pegs.append(None)\n",
    "    return list(zip(tokens, pos_pegs))\n",
    "\n",
    "def lemmatize(pos_tagged_tokens):\n",
    "    lemmatized_tokens = []\n",
    "    for i, (token, pos) in enumerate(pos_tagged_tokens):\n",
    "        #print(token, pos)\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token, pos=(pos if pos is not None else 'n')))\n",
    "    return lemmatized_tokens\n",
    "punctuation = [\"!\",\"%\",\"&\",\"(\",\")\",\"+\",\".\",\":\",\";\",\"<\",\"=\",\">\",\"?\",\"*\",\",\",\"\\t\",\"\",\"\\n\",'\"','-','/']\n",
    "\n",
    "def buildDic(ran,filename):\n",
    "    dictionary = {}\n",
    "    n = 0\n",
    "    for i in range(ran):\n",
    "        for line in open('data/'+filename+'/' + str(i) + '.txt'):\n",
    "            for ttt in range(len(line.strip().split('.')[0].split())):\n",
    "                t = True\n",
    "                word = line.strip().split('.')[0].split()[ttt]\n",
    "                c = dealWord(word)\n",
    "                if not c:\n",
    "                    continue\n",
    "                if c:\n",
    "                    dictionary = dictionary.get(c, 0) +1       \n",
    "    return dictionary\n",
    "\n",
    "def dealWord(word):\n",
    "    j = 0\n",
    "    while j < len(word):\n",
    "        if word[j] in punctuation:\n",
    "            word = word[:j]+word[j+1:]\n",
    "        else:\n",
    "            j+=1\n",
    "    if not word:\n",
    "        return ''\n",
    "    pos_tagged_tokens = get_pos([word.lower()])\n",
    "    c = lemmatize(pos_tagged_tokens)[0]\n",
    "    if c in nltk.corpus.stopwords.words(\"english\"):\n",
    "        return ''\n",
    "    if c in stop_words:\n",
    "        return \"\"\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112104"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = [\"./train/neg/\", \"./train/pos/\", \"./test/neg/\", \"./test/pos/\"]\n",
    "dictionary = {}\n",
    "for n in range(4):\n",
    "    files_tn = os.listdir(path[n])\n",
    "    for i in range(len(files_tn)):\n",
    "        for line in open(path[n] + files_tn[i]):\n",
    "            word_line = line.split()\n",
    "            for word in word_line:\n",
    "                c = dealWord(word)\n",
    "                if not c:\n",
    "                    continue\n",
    "                if c:\n",
    "                    dictionary[c] = dictionary.get(c, 0) + 1    \n",
    "\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building bag of words feature\n",
    "path = [\"./train/neg/\", \"./train/pos/\"]\n",
    "Key = sorted(list(dictionary.keys()), key = lambda x: dictionary[x], reverse = True)[:500]\n",
    "data_all = np.zeros((20000, 500))\n",
    "for n in range(2):\n",
    "    files_tn = os.listdir(path[n])\n",
    "    for i in range(10000):\n",
    "        for line in open(path[n] + files_tn[i]):\n",
    "            word_line = line.split()\n",
    "            for word in word_line:\n",
    "                c = dealWord(word)\n",
    "                if c in Key:\n",
    "                    data_all[(n == 1) * 10000 + i, Key.index(c)] += 1\n",
    "                    \n",
    "#pd.DataFrame(data_all).to_csv(\"data.csv\")\n",
    "data_all_df = pd.DataFrame(data_all, columns=Key)\n",
    "\n",
    "train_data = data_all[:9000,:]\n",
    "train_data = np.append(train_data, data_all[10000:19000,:], axis = 0)\n",
    "validation_data = data_all[9000:10000,:]\n",
    "validation_data = np.append(validation_data, data_all[19000:,:], axis = 0)\n",
    "train_label = np.array([0]*9000 + [1]*9000)\n",
    "test_label = np.array([0]*1000 + [1] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,2.0,2.0,1.0,...|  0.0|\n",
      "|[0.0,2.0,1.0,0.0,...|  0.0|\n",
      "|[3.0,1.0,0.0,0.0,...|  0.0|\n",
      "|[1.0,0.0,0.0,0.0,...|  0.0|\n",
      "|[2.0,3.0,0.0,1.0,...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process data to use Spark\n",
    "train = np.append(train_data.T, train_label.reshape(1,18000), axis = 0).T\n",
    "validation = np.append(validation_data.T, test_label.reshape(1,2000), axis = 0).T\n",
    "np.savetxt(\"train3.csv\", train, delimiter=\",\")\n",
    "np.savetxt(\"validation3.csv\", validation, delimiter=\",\")\n",
    "train_rdd = sc.textFile(\"./train3.csv\").map(lambda x: x.split(','))\n",
    "train_df = train_rdd.toDF()\n",
    "#train_df.show(10)\n",
    "train_new = train_df.withColumn('_1',train_df['_1'].cast(\"double\"))\n",
    "for i in range(2,502):\n",
    "    train_new = train_new.withColumn('_' + str(i),train_df['_' + str(i)].cast(\"double\"))\n",
    "test_new = test_df.withColumn('_1',test_df['_1'].cast(\"double\"))\n",
    "for i in range(2,502):\n",
    "    test_new = test_new.withColumn('_' + str(i),test_df['_' + str(i)].cast(\"double\"))\n",
    "row = Row('features','label')\n",
    "tr_df = train_new.rdd.map(lambda r: (row(DenseVector(r[:-1]),r[-1]))).toDF()\n",
    "te_df = test_new.rdd.map(lambda r: (row(DenseVector(r[:-1]),r[-1]))).toDF()\n",
    "te_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8725075"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest in Spark\n",
    "randomForest = RandomForestClassifier(numTrees=100, labelCol='label',featuresCol=\"features\").fit(tr_df)\n",
    "predictions = randomForest.transform(te_df)\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
