{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext \n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SQLContext as sqlCtx\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 1: Group Duplicates To Reduce Data Size\n",
    "root = \"./track2/\"\n",
    "trainPath = root + \"training.txt\"\n",
    "trainRDD = sc.textFile(trainPath,2)\n",
    "title = [\"Click\", \"Impression\", \"DisplayURl\", \"AdID\", \"AdvertiseId\", \"Depth\", \"Position\", \"QueryID\", \"KeywordID\", \"TitleID\", \"DescriptionID\", \"UserID\"]\n",
    "train = trainRDD.map(lambda x : x.split(\"\\t\"))\n",
    "trainDF = train.toDF(title)\n",
    "trainEX = trainDF[[\"Click\",\"Impression\",\"AdID\", \"QueryID\", \"Depth\", \"Position\", \"UserID\"]]\n",
    "trainU = (trainEX.withColumn(\"Click\",trainEX[\"Click\"].cast(\"double\"))\n",
    "            .withColumn(\"Impression\",trainEX[\"Impression\"].cast(\"double\"))\n",
    "            .withColumn(\"AdID\",trainEX[\"AdID\"].cast(\"double\"))\n",
    "            .withColumn(\"QueryID\",trainEX[\"QueryID\"].cast(\"double\"))\n",
    "            .withColumn(\"Depth\",trainEX[\"Depth\"].cast(\"double\"))\n",
    "            .withColumn(\"Position\",trainEX[\"Position\"].cast(\"double\"))\n",
    "            )\n",
    "trainDF = trainU.groupby(\"AdID\", \"QueryID\", \"Depth\", \"Position\",\"UserID\").agg({\"Click\": \"sum\", \"Impression\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 2: Non-big-data top 25,000 Frequent From Trainning Data\n",
    "# After ranking the frenquency, the first group has more than 25,000 instances.\n",
    "# Here I'm taking the 250 instances from the first 100 groups to make 25,000 instances.\n",
    "trainDFf = (trainDF.withColumn(\"Click\",trainDF[\"sum(Click)\"].cast(\"double\"))\n",
    "            .withColumn(\"Impression\",trainDF[\"sum(Impression)\"].cast(\"double\"))\n",
    "            .withColumn(\"AdID\",trainDF[\"AdID\"].cast(\"double\"))\n",
    "            .withColumn(\"QueryID\",trainDF[\"QueryID\"].cast(\"double\"))\n",
    "            .withColumn(\"Depth\",trainDF[\"Depth\"].cast(\"double\"))\n",
    "            .withColumn(\"Position\",trainDF[\"Position\"].cast(\"double\"))\n",
    "          .withColumn(\"UserID\",trainDF[\"UserID\"].cast(\"double\"))\n",
    "            )\n",
    "trainFQ = trainDFf.groupBy([\"AdID\", \"QueryID\"]).count()\n",
    "trainCT = trainDFf.join(trainFQ, [\"AdID\", \"QueryID\"], \"outer\")\n",
    "trainORD = trainCT.sort(\"count\", ascending = False)\n",
    "trainRK = trainORD.withColumn(\"rank\", denseRK().over(Window.partitionBy(\"count\").orderBy(desc(\"UserID\"))))\n",
    "trainF = trainRK.filter(col(\"rank\") <= 250)\n",
    "trainDFF = trainF.limit(25000)\n",
    "trainF.limit(25000).write.csv(\"train_df_f.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+\n",
      "|Depth|Position|       pCTR|\n",
      "+-----+--------+-----------+\n",
      "|  2.0|     1.0|0.053358235|\n",
      "|  1.0|     1.0|0.039917249|\n",
      "|  2.0|     2.0|0.026754174|\n",
      "|  3.0|     1.0|0.034008055|\n",
      "|  3.0|     3.0| 0.01078655|\n",
      "|  3.0|     2.0|0.016526995|\n",
      "+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 3: CTR\n",
    "pdNOR = trainDF.groupby(\"Depth\", \"Position\").agg({\"Click\": \"sum\", \"Impression\": \"sum\"})\n",
    "pd = pdNOR.withColumn(\"pCTR\", pdNOR[\"sum(Click)\"] / pdNOR[\"sum(Impression)\"])\n",
    "positionNM = pd[\"Depth\", \"Position\", \"pCTR\"]\n",
    "positionNM = (positionNM.withColumn(\"Depth\", positionNM[\"Depth\"].cast(\"double\"))\n",
    "              .withColumn(\"Position\", positionNM[\"Position\"].cast(\"double\"))\n",
    "              .withColumn(\"pCTR\", positionNM[\"pCTR\"].cast(\"double\")))\n",
    "positionNM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----+--------+-----------+----------+-----+\n",
      "|       AdID|QueryID|Depth|Position|     UserID|Impression|Click|\n",
      "+-----------+-------+-----+--------+-----------+----------+-----+\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3907341E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3907195E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3907049E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3906018E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3905449E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3904198E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3903189E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3902876E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3902259E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3901976E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3900019E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3898575E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3898203E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3897844E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3897579E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3897359E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3896047E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3893913E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3893308E7|       1.0|  0.0|\n",
      "|2.1522776E7|    0.0|  1.0|     1.0|2.3892615E7|       1.0|  0.0|\n",
      "+-----------+-------+-----+--------+-----------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tRDD = sc.textFile(\"./trainDFF.csv\").map(lambda x : x.split(\",\"))\n",
    "tRDD = get trainDF\n",
    "trainDFF = tRDD.toDF([\"AdID\", \"QueryID\", \"Depth\", \"Position\", \"UserID\", \"Impression\", \"Click\"])\\\n",
    "[[\"AdID\", \"QueryID\", \"Depth\", \"Position\", \"UserID\", \"Impression\", \"Click\"]]\n",
    "trainDFF = (trainDFF.withColumn(\"Click\", trainDFF[\"Click\"].cast(\"double\"))\n",
    "            .withColumn(\"Impression\", trainDFF[\"Impression\"].cast(\"double\"))\n",
    "            .withColumn(\"AdID\", trainDFF[\"AdID\"].cast(\"double\"))\n",
    "            .withColumn(\"QueryID\", trainDFF[\"QueryID\"].cast(\"double\"))\n",
    "            .withColumn(\"Depth\", trainDFF[\"Depth\"].cast(\"double\"))\n",
    "            .withColumn(\"Position\", trainDFF[\"Position\"].cast(\"double\"))\n",
    "            .withColumn(\"UserID\", trainDFF[\"UserID\"].cast(\"double\")))\n",
    "trainDFF.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDFF.count()\n",
    "pdNOR = trainDF.groupby(\"Depth\", \"Position\").agg({\"Click\": \"sum\", \"Impression\": \"sum\"})\n",
    "pd = pdNOR.withColumn(\"pCTR\",pdNOR[\"sum(Click)\"] / pdNOR[\"sum(Impression)\"])\n",
    "positionNM = pd[(\"Depth\", \"Position\", \"pCTR\")]\n",
    "trainDFPRENOR = trainDFF.join(positionNM, [\"Depth\", \"Position\"],\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+-------+-----------+----------+-----+-----------+\n",
      "|Depth|Position|       AdID|QueryID|     UserID|Impression|Click|       pCTR|\n",
      "+-----+--------+-----------+-------+-----------+----------+-----+-----------+\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3907341E7|       1.0|  0.0|0.039917249|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3907195E7|       1.0|  0.0|0.039917249|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3907049E7|       1.0|  0.0|0.039917249|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3906018E7|       1.0|  0.0|0.039917249|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3905449E7|       1.0|  0.0|0.039917249|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3904198E7|       1.0|  0.0|0.039917249|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3903189E7|       1.0|  0.0|0.039917249|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3902876E7|       1.0|  0.0|0.039917249|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3902259E7|       1.0|  0.0|0.039917249|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3901976E7|       1.0|  0.0|0.039917249|\n",
      "+-----+--------+-----------+-------+-----------+----------+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDFPRENOR.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+-------+-----------+----------+-----+-----------+----------+\n",
      "|Depth|Position|       AdID|QueryID|     UserID|Impression|Click|       pCTR|normed_CTR|\n",
      "+-----+--------+-----------+-------+-----------+----------+-----+-----------+----------+\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3907341E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3907195E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3907049E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3906018E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3905449E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3904198E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3903189E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3902876E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3902259E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "|  1.0|     1.0|2.1522776E7|    0.0|2.3901976E7|       1.0|  0.0|0.039917249|       0.0|\n",
      "+-----+--------+-----------+-------+-----------+----------+-----+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDFPRENOR\n",
    "trainDF = trainDFPRENOR.withColumn(\"normedCTR\", trainDFPRENOR[\"Click\"] / (trainDFPRENOR[\"Impression\"] * trainDFPRENOR[\"pCTR\"]))\n",
    "trainDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = trainDF.filter(trainDF[\"normedCTR\"] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 5 Features\n",
    "userRDD = sc.textFile(root + \"userid_profile.txt\").map(lambda x : x.split(\"\\t\"))\n",
    "userDF = userRDD.toDF([\"UserID\", \"Gender\", \"Age\"])\n",
    "userDF = (userDF.withColumn(\"UserID\",userDF[\"UserID\"].cast(\"double\"))\n",
    "          .withColumn(\"Gender\",userDF[\"Gender\"].cast(\"double\"))\n",
    "          .withColumn(\"Age\",userDF[\"Age\"].cast(\"double\")))\n",
    "trainP5 = trainDF.join(userDF,['UserID'], \"outer\")\n",
    "trainF = trainP5.filter(trainP5[\"Position\"] > 0)\n",
    "trainData = trainF.drop(\"pCTR\").limit(25000)\n",
    "trainData.write.csv(\"train_data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Data\n",
    "testRRD = sc.textFile(root + \"test.txt\").map(lambda x:x.split(\"\\t\"))\n",
    "title = [\"Click\",\"Impression\", \"DisplayURl\", \"AdID\", \"AdvertiseId\", \"Depth\", \"Position\", \"QueryID\", \"KeywordID\",\"TitleID\",\"DescriptionID\", \"UserID\"]\n",
    "testDF = testRRD.toDF(title[2:])\n",
    "testDF = testDF.drop(\"DisplayURl\", \"AdvertiseId\",\"KeywordID\",\"TitleID\",\"DescriptionID\")\n",
    "testDF = (testDF.withColumn(\"AdID\",testDF[\"AdID\"].cast(\"double\"))\n",
    "          .withColumn(\"QueryID\",testDF[\"QueryID\"].cast(\"double\"))\n",
    "          .withColumn(\"Depth\",testDF[\"Depth\"].cast(\"double\"))\n",
    "          .withColumn(\"Position\",testDF[\"Position\"].cast(\"double\"))\n",
    "          .withColumn(\"UserID\",testDF[\"UserID\"].cast(\"double\")))\n",
    "testU = testDF.join(userDF,[\"UserID\"], \"outer\")\n",
    "test = testU.filter(testU[\"Position\"] > 0)\n",
    "test.write.csv(\"test_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
