{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.classification import NaiveBayes, MultilayerPerceptronClassifier\n",
    "import time\n",
    "%pylab inline\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load csv file as RDD\n",
    "trainRDD = sc.textFile(\"train.csv\")\n",
    "testRDD = sc.textFile(\"test.csv\")\n",
    "#print(trainRDD.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert RDD to DF\n",
    "def convert(rdd):\n",
    "    header = rdd.first()\n",
    "    body = rdd.filter(lambda r:r!=header)\n",
    "    def rowHelper(row):\n",
    "        l = row.replace('\"', '').split(\",\")\n",
    "        t = tuple(l)\n",
    "        return t\n",
    "    \n",
    "    parsed = body.map(rowHelper)\n",
    "    headers = header.split(\",\")\n",
    "    if headers[1] == \"Survived\":\n",
    "        headers.insert(3,\"FirstName\")\n",
    "    else:\n",
    "        headers.insert(2,\"FirstName\")\n",
    "        \n",
    "    return parsed.toDF(headers)\n",
    "\n",
    "train = convert(trainRDD)\n",
    "test = convert(testRDD)\n",
    "#print(train.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: double (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: double (nullable = true)\n",
      " |-- Parch: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Datatype: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine Data\n",
    "train = train.withColumn(\"Datatype\", lit(\"train\"))\n",
    "test = test.withColumn(\"Datatype\",lit(\"test\")).withColumn(\"Survived\", lit(0))\n",
    "test = test[train.columns]\n",
    "combined = train.unionAll(test)\n",
    "\n",
    "# Drop Columns\n",
    "toDrop = [\"FirstName\", \"PassengerId\", \"Ticket\", \"Cabin\"]\n",
    "for i in toDrop:\n",
    "    combined = combined.drop(i)\n",
    "\n",
    "# Convert To Double\n",
    "combined = (combined.withColumn(\"Age\", combined[\"Age\"].cast(\"double\"))\n",
    "            .withColumn(\"SibSp\", combined[\"SibSp\"].cast(\"double\"))\n",
    "            .withColumn(\"Parch\", combined[\"Parch\"].cast(\"double\"))\n",
    "            .withColumn(\"Fare\", combined[\"Fare\"].cast(\"double\"))\n",
    "            .withColumn(\"Survived\", combined[\"Survived\"].cast(\"double\")))\n",
    "combined.printSchema()\n",
    "\n",
    "# Handle Missing Values\n",
    "ageMean = combined.groupBy().mean(\"Age\").first()[0]\n",
    "fareMean = combined.groupBy().mean(\"Fare\").first()[0]\n",
    "combined = combined.na.fill({\"Age\":ageMean,\"Fare\":fareMean,\"Embarked\":\"S\"})\n",
    "#print(combined.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Name & Title\n",
    "titles = udf(lambda name: name.split(\".\")[0].strip(),StringType())\n",
    "combined = combined.withColumn(\"Title\", titles(combined[\"Name\"]))\n",
    "combined = combined.withColumn(\"Title\", regexp_replace(\"Title\", \"Mlle\", \"Miss\"))\n",
    "combined = combined.withColumn(\"Title\", regexp_replace(\"Title\", \"Ms\", \"Miss\"))\n",
    "combined = combined.withColumn(\"Title\", regexp_replace(\"Title\", \"Mme\", \"Mrs\"))\n",
    "for i in [\"Lady\", \"Countess\",\"Capt\", \"Col\",\"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"]:\n",
    "    combined = combined.withColumn(\"Title\", regexp_replace(\"Title\", i, \"Others\"))\n",
    "#print(combined.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Age\n",
    "combined = combined.withColumn(\"AgeRange\", F.when(combined[\"Age\"] < 16, lit(0)))\n",
    "combined = combined.withColumn(\"AgeRange\", F.when((combined[\"Age\"] >= 16) & (combined[\"Age\"] < 36), lit(1)).otherwise(combined[\"AgeRange\"]))\n",
    "combined = combined.withColumn(\"AgeRange\", F.when(combined[\"Age\"] >= 36, lit(2)).otherwise(combined[\"AgeRange\"]))\n",
    "#print(combined.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Party Size\n",
    "combined = combined.withColumn(\"PartySize\", F.when((combined[\"SibSp\"] + combined[\"Parch\"] == 0), lit(0)))\n",
    "combined = combined.withColumn(\"PartySize\", F.when(((combined[\"SibSp\"] + combined[\"Parch\"] > 0) & (combined[\"SibSp\"] + combined[\"Parch\"] <= 3)), lit(1)).otherwise(combined[\"PartySize\"]))\n",
    "combined = combined.withColumn(\"PartySize\", F.when(((combined[\"SibSp\"] + combined[\"Parch\"] > 3)), lit(2)).otherwise(combined[\"PartySize\"]))\n",
    "#print(combined.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fare\n",
    "combined = combined.withColumn(\"FareRange\", F.when(combined[\"Fare\"] <= 7.91, lit(0)))\n",
    "combined = combined.withColumn(\"FareRange\",F.when((combined[\"Fare\"] > 7.91) & (combined[\"Fare\"] <= 14.454), lit(1)).otherwise(combined[\"FareRange\"]))\n",
    "combined = combined.withColumn(\"FareRange\",F.when((combined[\"Fare\"] > 14.454) & (combined[\"Fare\"] <= 31), lit(2)).otherwise(combined[\"FareRange\"]))\n",
    "combined = combined.withColumn(\"FareRange\",F.when(combined[\"Fare\"]> 31, lit(3)).otherwise(combined[\"FareRange\"]))\n",
    "#print(combined.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Index Features\n",
    "cats = [\"Pclass\", \"Title\", \"Sex\", \"Embarked\"]\n",
    "def indexer(data, col):\n",
    "    res = StringIndexer(inputCol = col, outputCol = col + \"I\").fit(data)\n",
    "    return res\n",
    "\n",
    "indexers = [indexer(combined, col) for col in cats]\n",
    "pipeline = Pipeline(stages = indexers)\n",
    "combined = pipeline.fit(combined).transform(combined)\n",
    "for i in [\"Pclass\", \"Title\", \"Sex\", \"Embarked\", \"Fare\", \"SibSp\", \"Parch\", \"Age\", \"Name\"]:\n",
    "    combined = combined.drop(i)\n",
    "#print(combined.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------------+\n",
      "|datatype|label|            features|\n",
      "+--------+-----+--------------------+\n",
      "|   train|  0.0|[1.0,1.0,0.0,0.0,...|\n",
      "|   train|  1.0|[2.0,1.0,3.0,1.0,...|\n",
      "|   train|  1.0|[1.0,0.0,1.0,0.0,...|\n",
      "|   train|  1.0|[1.0,1.0,3.0,1.0,...|\n",
      "|   train|  0.0|[1.0,0.0,1.0,0.0,...|\n",
      "+--------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert To Label/Features Vector\n",
    "nums = [\"Survived\", \"AgeRange\", \"PartySize\", \"FareRange\"]\n",
    "catsI = [i + \"I\" for i in cats]\n",
    "featCol = nums + catsI\n",
    "featCol.remove(\"Survived\")\n",
    "labelCol = [\"Datatype\", \"Survived\"]\n",
    "row = Row(\"datatype\", \"label\", \"features\")\n",
    "\n",
    "combined = combined[labelCol + featCol]\n",
    "lf = (combined.rdd.map(lambda r : (row(r[0], r[1], DenseVector(r[2:])))).toDF())\n",
    "#lf = (StringIndexer(inputCol = \"label\", outputCol = \"index\").fit(lf).transform(lf))\n",
    "#lf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train Row: 634\n",
      "# Validate Row: 135\n",
      "# Total Row: 891\n"
     ]
    }
   ],
   "source": [
    "# seperate train/test data\n",
    "trainAll = lf.where(lf.datatype == \"train\")\n",
    "test = lf.where(lf.datatype == \"test\")\n",
    "\n",
    "# Select A Partial of Training Data\n",
    "#trainLimited, hold = trainAll.randomSplit([0.88, 0.12])\n",
    "\n",
    "# random split further to get train/validate\n",
    "train, validate = trainLimited.randomSplit([0.8, 0.2])\n",
    "evl = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "#print(\"# Train Row: \" + str(train.count()))\n",
    "#print(\"# Validate Row: \" + str(validate.count()))\n",
    "#print(\"# Total Row: \" + str(trainAll.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of Data Processing: 64.7177131176\n"
     ]
    }
   ],
   "source": [
    "delta = time.time() - start\n",
    "print(\"Time of Data Processing: \" + str(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 0.864155431362 0.866523508137\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "start = time.time()\n",
    "log = LogisticRegression(labelCol=\"label\").fit(train)\n",
    "pred1 = log.transform(train)\n",
    "pred2 = log.transform(validate)\n",
    "delta = time.time() - start\n",
    "print(\"Logistic Regression \" + str(evl.evaluate(pred1)) + \" \" + str(evl.evaluate(pred2)) + \" Time: \" + str(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 0.852646479955 0.819846292948 Time: 0.811095952988\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "start = time.time()\n",
    "dtree = DecisionTreeClassifier(labelCol=\"label\").fit(train)\n",
    "pred1 = dtree.transform(train)\n",
    "pred2 = dtree.transform(validate)\n",
    "delta = time.time() - start\n",
    "print(\"Decision Tree \" + str(evl.evaluate(pred1)) + \" \" + str(evl.evaluate(pred2)) + \" Time: \" + str(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest 0.902266005704 0.869914104882 Time: 0.806475162506\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "start = time.time()\n",
    "randForest = RandomForestClassifier(labelCol=\"label\").fit(train)\n",
    "pred1 = randForest.transform(train)\n",
    "pred2 = randForest.transform(validate)\n",
    "delta = time.time() - start\n",
    "print(\"Random Forest \" + str(evl.evaluate(pred1)) + \" \" + str(evl.evaluate(pred2)) + \" Time: \" + str(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kaggle Submission\n",
    "#rf = RandomForestClassifier(labelCol=\"label\").fit(trainAll)\n",
    "#pred = randForest.transform(test)\n",
    "#pred.rdd.map(lambda x: \",\".join(map(str, x))).coalesce(1).saveAsTextFile(\"a.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
